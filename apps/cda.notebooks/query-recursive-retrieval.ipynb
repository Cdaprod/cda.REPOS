{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tQvt1ypQTPO"
      },
      "source": [
        "## Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52paLw85uwIQ",
        "outputId": "06aef162-e393-420d-cff8-2934dc700f9a"
      },
      "outputs": [],
      "source": [
        "!pip install weaviate-client llama-index==0.8.10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raf0H4KtQV-5"
      },
      "source": [
        "## Connect to Weaviate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxsNzkIFQXXb",
        "outputId": "57a46e6d-ed31-42fd-fa36-4b6848c845b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Started /root/.cache/weaviate-embedded: process ID 4518\n"
          ]
        }
      ],
      "source": [
        "import weaviate\n",
        "client = weaviate.Client(\n",
        "    embedded_options = weaviate.embedded.EmbeddedOptions()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A91WC4jzQeEI"
      },
      "source": [
        "### Create Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9QnOlvjQfg8",
        "outputId": "5dc122ff-7955-4c1a-828b-daffa1a069bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Schema was created.\n"
          ]
        }
      ],
      "source": [
        "schema = {\n",
        "    \"classes\": [\n",
        "        {\n",
        "            \"class\": \"WeaviateBlogPost\",\n",
        "            \"description\": \"Blog post from the Weaviate website.\",\n",
        "            \"vectorizer\": \"text2vec-openai\",\n",
        "            \"properties\": [\n",
        "                {\n",
        "                    \"name\": \"content\",\n",
        "                    \"dataType\": [\"text\"],\n",
        "                    \"description\": \"Content from the blog post.\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"class\": \"HuggingFaceBlogPost\",\n",
        "            \"description\": \"Blog post from the HuggingFace website.\",\n",
        "            \"vectorizer\": \"text2vec-openai\",\n",
        "            \"properties\": [\n",
        "                {\n",
        "                    \"name\": \"content\",\n",
        "                    \"dataType\": [\"text\"],\n",
        "                    \"description\": \"Content from the blog post.\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "client.schema.create(schema)\n",
        "print(\"Schema was created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pMof9epRbE-"
      },
      "source": [
        "### Load in Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_gHO-DxZsL1x"
      },
      "outputs": [],
      "source": [
        "from llama_index import download_loader, SimpleWebPageReader\n",
        "from llama_index.node_parser import SimpleNodeParser\n",
        "from llama_index.vector_stores import WeaviateVectorStore\n",
        "from llama_index import VectorStoreIndex\n",
        "from llama_index.storage.storage_context import StorageContext\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"sk-key\"\n",
        "\n",
        "SimpleWebPageReader = download_loader(\"SimpleWebPageReader\")\n",
        "\n",
        "loader = SimpleWebPageReader(html_to_text=True)\n",
        "WeaviateBlog = loader.load_data(urls=['https://weaviate.io/blog/pq-rescoring'])\n",
        "WeaviateBlog_vector_store = WeaviateVectorStore(weaviate_client=client, index_name=\"WeaviateBlogPost\", text_key=\"content\")\n",
        "WeaviateBlog_storage_context = StorageContext.from_defaults(vector_store=WeaviateBlog_vector_store)\n",
        "WeaviateBlogIndex = VectorStoreIndex.from_documents(WeaviateBlog, storage_context=WeaviateBlog_storage_context)\n",
        "\n",
        "HuggingFaceBlog = loader.load_data(urls=['https://huggingface.co/blog/ram-efficient-pytorch-fsdp'])\n",
        "HuggingFaceBlog_vector_store = WeaviateVectorStore(weaviate_client=client, index_name=\"HuggingFaceBlogPost\", text_key=\"content\")\n",
        "HuggingFaceBlog_storage_context = StorageContext.from_defaults(vector_store=HuggingFaceBlog_vector_store)\n",
        "HuggingFaceBlogIndex = VectorStoreIndex.from_documents(HuggingFaceBlog, storage_context=HuggingFaceBlog_storage_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4OZ8f7GRe5f"
      },
      "source": [
        "## Create Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ddTD13Z1wkzb"
      },
      "outputs": [],
      "source": [
        "from llama_index.schema import IndexNode\n",
        "\n",
        "summaries = {\n",
        "    \"Weaviate\": \"This node provides blog posts from Weaviate, a Vector Database.\",\n",
        "    \"HuggingFace\": \"This node provides blog posts from HuggingFace, tools for training Machine Learning models.\"\n",
        "}\n",
        "\n",
        "df_nodes = [\n",
        "    IndexNode(text=summaries[\"Weaviate\"], index_id=\"WeaviateBlogs\"),\n",
        "    IndexNode(text=summaries[\"HuggingFace\"], index_id=\"HuggingFaceBlogs\")\n",
        "]\n",
        "\n",
        "WeaviateBlogQueryEngine = WeaviateBlogIndex.as_query_engine()\n",
        "HuggingFaceBlogQueryEngine = HuggingFaceBlogIndex.as_query_engine()\n",
        "\n",
        "df_id_query_engine_mapping = {\n",
        "    \"WeaviateBlogs\": WeaviateBlogQueryEngine,\n",
        "    \"HuggingFaceBlogs\": HuggingFaceBlogQueryEngine\n",
        "}\n",
        "\n",
        "Tool_Description_Index = VectorStoreIndex(df_nodes)\n",
        "Tool_Retriever = Tool_Description_Index.as_retriever(similarity_top_k=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpUKO2MHR6PP"
      },
      "source": [
        "## Build Recursive Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OWWMu5BJ0HgQ"
      },
      "outputs": [],
      "source": [
        "from llama_index.retrievers import RecursiveRetriever\n",
        "from llama_index.query_engine import RetrieverQueryEngine\n",
        "from llama_index.response_synthesizers import get_response_synthesizer\n",
        "\n",
        "recursive_retriever = RecursiveRetriever(\n",
        "    \"vector\",\n",
        "    retriever_dict={\"vector\": Tool_Retriever},\n",
        "    query_engine_dict=df_id_query_engine_mapping,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "response_synthesizer = get_response_synthesizer(\n",
        "    response_mode=\"compact\"\n",
        ")\n",
        "\n",
        "query_engine = RetrieverQueryEngine.from_args(\n",
        "    recursive_retriever, response_synthesizer=response_synthesizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdV8cjaP0edt",
        "outputId": "1d780e01-aa2b-47dd-d8ef-8ecde416c657"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36;1m\u001b[1;3mRetrieving with query id None: What is Product Quantization?\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3mRetrieved node with id, entering: WeaviateBlogs\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mRetrieving with query id WeaviateBlogs: What is Product Quantization?\n",
            "\u001b[0m\u001b[32;1m\u001b[1;3mGot response: Product Quantization is a method used to compress vectors, which helps to reduce memory requirements. It works by representing vectors in a more compact format, sacrificing some accuracy in order to save memory. This compression technique is often used in applications where memory efficiency is crucial.\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"What is Product Quantization?\").response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5Il4CHb0hGI",
        "outputId": "e5c170d1-c489-43ed-ed57-3f43b6b83053"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36;1m\u001b[1;3mRetrieving with query id None: What does FSDP do?\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3mRetrieved node with id, entering: HuggingFaceBlogs\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mRetrieving with query id HuggingFaceBlogs: What does FSDP do?\n",
            "\u001b[0m\u001b[32;1m\u001b[1;3mGot response: FSDP, or Fully Sharded Data Parallelism, is a paradigm in which the optimizer states, gradients, and parameters are sharded across devices. During the forward pass, each FSDP unit performs an all-gather operation to get the complete weights, followed by computation and discarding the shards from other devices. In the backward pass, each FSDP unit performs an all-gather operation to get the complete weights, with computation performed to get the local gradients. These local gradients are then averaged and sharded across the devices via a reduce-scatter operation so that each device can update the parameters of its shard. FSDP enables efficient training of large models in a multi-node, multi-GPU setting.\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"What does FSDP do?\").response"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
