{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVo_25Tge76N"
      },
      "source": [
        "## Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mBoH4EYA0oGq"
      },
      "outputs": [],
      "source": [
        "!pip install llama_index llama_hub weaviate_client urllib3 llama-cpp-python > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dYmRkpOt1u7q"
      },
      "outputs": [],
      "source": [
        "from llama_index import (\n",
        "    VectorStoreIndex,\n",
        "    ServiceContext,\n",
        "    StorageContext,\n",
        "    SQLDatabase,\n",
        "    download_loader\n",
        ")\n",
        "from llama_index.vector_stores import WeaviateVectorStore\n",
        "\n",
        "import weaviate\n",
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnsDxF4ifBmI"
      },
      "source": [
        "## Connect to Weaviate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp1xmGEr2Z0E",
        "outputId": "7709f5a5-661f-4876-da68-d47597103aab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Binary /root/.cache/weaviate-embedded did not exist. Downloading binary from https://github.com/weaviate/weaviate/releases/download/v1.21.1/weaviate-v1.21.1-Linux-amd64.tar.gz\n",
            "Started /root/.cache/weaviate-embedded: process ID 1699\n"
          ]
        }
      ],
      "source": [
        "client = weaviate.Client(\n",
        "    embedded_options=weaviate.embedded.EmbeddedOptions()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDIHyiyPfL8I"
      },
      "source": [
        "### Create Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upanknG02liC",
        "outputId": "a5f2a230-aadd-4358-d59f-a75fd3bec306"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Podcast schema was created.\n"
          ]
        }
      ],
      "source": [
        "podcast_schema = {\n",
        "   \"classes\": [\n",
        "       {\n",
        "           \"class\": \"Podcast\",\n",
        "           \"description\": \"Weaviate podcast\",\n",
        "           \"vectorizer\": \"text2vec-openai\",\n",
        "           \"properties\": [\n",
        "               {\n",
        "                  \"name\": \"Content\",\n",
        "                  \"dataType\": [\"text\"],\n",
        "                  \"description\": \"Content from the podcasts.\",\n",
        "               }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "client.schema.create(podcast_schema)\n",
        "print(\"Podcast schema was created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P2WVok5fQvr"
      },
      "source": [
        "## Load in Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EEBHP6L-2sUD"
      },
      "outputs": [],
      "source": [
        "from llama_index import download_loader\n",
        "\n",
        "YouTubeTranscriptReader = download_loader(\"YoutubeTranscriptReader\")\n",
        "\n",
        "loader = YouTubeTranscriptReader()\n",
        "podcasts = loader.load_data(ytlinks=['https://www.youtube.com/watch?v=xk28RMhRy1U', 'https://www.youtube.com/watch?v=Du6IphCcCec',\n",
        "'https://www.youtube.com/watch?v=Q7f2JeuMN7E', 'https://www.youtube.com/watch?v=nSCUk5pHXlo'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4kwMqZUftdy"
      },
      "source": [
        "## Build the Weaviate Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6F1LPp53QF1",
        "outputId": "c440c298-c635-4c12-95bf-817a48f8792c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:llama_index.vector_stores.weaviate:class_prefix is deprecated, please use index_name\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedded weaviate wasn't listening on port 6666, so starting embedded weaviate again\n",
            "Started /root/.cache/weaviate-embedded: process ID 3416\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "openai.api_key = \"sk-key\"\n",
        "\n",
        "vector_store = WeaviateVectorStore(weaviate_client=client, class_prefix=\"Podcasts_index\")\n",
        "\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "podcast_index = VectorStoreIndex.from_documents(podcasts, storage_context=storage_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4RMRgE8g4bE"
      },
      "source": [
        "## Create SQL Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ugdDS0DL3upy"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import (\n",
        "    create_engine,\n",
        "    MetaData,\n",
        "    Table,\n",
        "    Column,\n",
        "    String,\n",
        "    Integer,\n",
        "    select,\n",
        "    column,\n",
        ")\n",
        "\n",
        "engine = create_engine(\"sqlite:///:memory:\", future=True)\n",
        "metadata_obj = MetaData()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "a-5u5_WF3-2U"
      },
      "outputs": [],
      "source": [
        "table_name = \"podcast_stats\"\n",
        "podcast_stats_table = Table(\n",
        "    table_name,\n",
        "    metadata_obj,\n",
        "    Column(\"podcast_title\", String(16), primary_key=True),\n",
        "    Column(\"views\", Integer),\n",
        "    Column(\"duration\", Integer),\n",
        ")\n",
        "\n",
        "metadata_obj.create_all(engine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zWmZ3nV4YaJ",
        "outputId": "8929dda9-f9e8-4513-f793-5de43fe86427"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['podcast_stats'])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metadata_obj.tables.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "j-B-lF-T4Z2u"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import insert\n",
        "\n",
        "rows = [\n",
        "    {\"podcast_title\": \"Weaviate 1.20\", \"views\": 328, \"duration\": 65},\n",
        "    {\"podcast_title\": \"Weaviate 1.19\", \"views\": 280, \"duration\": 27},\n",
        "    {\"podcast_title\": \"Weaviate 1.18\", \"views\": 428, \"duration\": 65},\n",
        "    {\"podcast_title\": \"Weaviate 1.17\", \"views\": 257, \"duration\": 43}\n",
        "]\n",
        "\n",
        "for row in rows:\n",
        "  stmt = insert(podcast_stats_table).values(**row)\n",
        "  with engine.connect() as connection:\n",
        "    cursor = connection.execute(stmt)\n",
        "    connection.commit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create SQL Table in LlamaIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tOp9RGLt5tJk"
      },
      "outputs": [],
      "source": [
        "sql_database = SQLDatabase(engine, include_tables=[\"podcast_stats\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YtHjm93O5xvy"
      },
      "outputs": [],
      "source": [
        "from llama_index.indices.struct_store.sql_query import NLSQLTableQueryEngine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "eKK5nOj-50ve"
      },
      "outputs": [],
      "source": [
        "# set up text2SQL prompt\n",
        "sql_query_engine = NLSQLTableQueryEngine(\n",
        "    sql_database=sql_database,\n",
        "    tables=[\"podcast_stats\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JkVOaovf54RP"
      },
      "outputs": [],
      "source": [
        "vector_query_engine = podcast_index.as_query_engine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tell LlamaIndex about the Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "iszRogIX6LqJ"
      },
      "outputs": [],
      "source": [
        "from llama_index.tools.query_engine import QueryEngineTool\n",
        "\n",
        "sql_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine = sql_query_engine,\n",
        "    description=(\n",
        "        \"Useful for translating a natural language query into a SQL query over a table containing: \"\n",
        "        \"podcast_stats, containing the views/duration of each podcast\"\n",
        "    ),\n",
        ")\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=\"Useful for answering semantic questions about Weaviate release podcasts\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FtCm96Kf65om"
      },
      "outputs": [],
      "source": [
        "from llama_index.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.selectors.llm_selectors import LLMSingleSelector\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools=([sql_tool] + [vector_tool]),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxNnuoDC7FtX",
        "outputId": "ce5b2b87-7ea3-4cea-9c1f-2fa0a10f0285"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The podcast episode titled \"Weaviate 1.18\" had the most views with a total of 428 views.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"Which release podcast had the most views?\")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m8gkQM67MoV",
        "outputId": "ba88177b-4705-4716-8a33-00a537fcc25e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In Weaviate 1.20, a new feature called multi-tenancy was introduced. This feature allows users to separate and isolate their data within the application. For example, if a user has an application that indexes documents from their hard drive, they can ensure that only they have access to search through those documents and that other users cannot access them. Multi-tenancy in Weaviate helps to limit the vector space and allows for efficient searching and filtering of data. It provides a technical solution to the problem of managing a large graph of vectors spread across multiple tenants.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"Tell me about a new feature in Weaviate 1.20\")\n",
        "print(str(response))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
