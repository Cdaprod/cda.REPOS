{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVZDL5znJ3RC"
      },
      "source": [
        "## Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxdwy-dnJ6VL",
        "outputId": "72c0b436-f27c-4fd9-999b-76f3d761c934"
      },
      "outputs": [],
      "source": [
        "!pip install weaviate-client llama-index==0.8.10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIfQ1wv6J0Gj"
      },
      "source": [
        "## Connect to Weaviate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFZMb3ccJ0Gl",
        "outputId": "83ad3fc1-46a9-47e3-c989-8cb1c9f5eb3e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import weaviate\n",
        "\n",
        "client = weaviate.Client(\n",
        "    embedded_options=weaviate.embedded.EmbeddedOptions()\n",
        ")\n",
        "\n",
        "client.schema.get()  # Get the schema to test connection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noWioK-rJ0Gm"
      },
      "source": [
        "## Create Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snK23oHcJ0Gm",
        "outputId": "a36d0cb7-f716-4d0d-c26c-b706d52c1a03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Schema was created.\n"
          ]
        }
      ],
      "source": [
        "schema = {\n",
        "   \"classes\": [\n",
        "       {\n",
        "           \"class\": \"BlogPost\",\n",
        "           \"description\": \"Blog post from the Weaviate website.\",\n",
        "           \"vectorizer\": \"text2vec-openai\",\n",
        "           \"properties\": [\n",
        "               {\n",
        "                  \"name\": \"content\",\n",
        "                  \"dataType\": [\"text\"],\n",
        "                  \"description\": \"Content from the blog post\",\n",
        "               }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "client.schema.create(schema)\n",
        "\n",
        "print(\"Schema was created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqQhSEeIJ0Gm"
      },
      "source": [
        "## Load in Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "skahGlY3J0Gn"
      },
      "outputs": [],
      "source": [
        "from llama_index import download_loader, SimpleWebPageReader\n",
        "\n",
        "SimpleWebPageReader = download_loader(\"SimpleWebPageReader\")\n",
        "\n",
        "loader = SimpleWebPageReader(html_to_text=True)\n",
        "blog = loader.load_data(urls=['https://weaviate.io/blog/llamaindex-and-weaviate'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d60skGPPJ0Gn"
      },
      "source": [
        "## Construct Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmdoGWDjJ0Go",
        "outputId": "a06d411c-e7a4-4e34-9cba-4d432c7a35b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.vector_stores import WeaviateVectorStore\n",
        "from llama_index import VectorStoreIndex, StorageContext\n",
        "from llama_index.storage.storage_context import StorageContext\n",
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = 'sk-key'\n",
        "\n",
        "# construct vector store\n",
        "vector_store = WeaviateVectorStore(weaviate_client = client, index_name=\"BlogPost\", text_key=\"content\")\n",
        "\n",
        "# setting up the storage for the embeddings\n",
        "storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
        "\n",
        "# set up the index\n",
        "index = VectorStoreIndex(blog, storage_context = storage_context)\n",
        "\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpYh70tZJ0Go"
      },
      "source": [
        "## Set up Sub Question Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ce3sfYsiJ0Go"
      },
      "outputs": [],
      "source": [
        "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.query_engine import SubQuestionQueryEngine\n",
        "\n",
        "query_engine_tools = [\n",
        "    QueryEngineTool(\n",
        "        query_engine = query_engine,\n",
        "        metadata = ToolMetadata(name='BlogPost', description='Blog post about the integration of LlamaIndex and Weaviate')\n",
        "    )\n",
        "]\n",
        "\n",
        "query_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4egarphqJ0Go"
      },
      "source": [
        "## Query Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gf3lXZcJ0Go",
        "outputId": "76a00d11-bf44-4fb9-954c-1c54c2ea9c59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 3 sub questions.\n",
            "\u001b[36;1m\u001b[1;3m[BlogPost] Q: What is LlamaIndex?\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m[BlogPost] Q: What is Weaviate?\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m[BlogPost] Q: How does LlamaIndex integrate with Weaviate?\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m[BlogPost] A: Weaviate is a software system that combines a language model with an external storage provider to create a \"chat with your data\" experience. It can be used to build powerful and reliable retrieval-augmented generation (RAG) systems, which enable the language model to access and retrieve specific facts, figures, or contextually relevant information. Weaviate can be used in various applications such as search engines and chatbots. It provides a vector database for storing and indexing data, and it can be integrated with other data frameworks like LlamaIndex to facilitate data management and query modules.\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m[BlogPost] A: LlamaIndex integrates with Weaviate by providing the critical components needed to easily set up a powerful and reliable retrieval-augmented generation (RAG) stack. Weaviate acts as the vector database, which serves as the external storage provider. LlamaIndex, on the other hand, offers a data framework for building large language model (LLM) applications. It provides tools for data ingestion, data indexing, and data querying. With LlamaIndex, users can easily manage and orchestrate their data in Weaviate when building LLM applications such as search engines and chatbots.\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m[BlogPost] A: LlamaIndex is a data framework for building large language model (LLM) applications. It provides a comprehensive toolkit for data ingestion, management, and querying. LlamaIndex offers connectors to various data sources, such as file formats, APIs, and web scrapers, making it easy to integrate data from existing files and applications. It also supports indexing unstructured, semi-structured, and structured data, allowing users to split source documents into text chunks and store them in a vector database. Additionally, LlamaIndex provides tools for defining an advanced retrieval/query engine over the data, enabling tasks like semantic search, structured analytics, and query decomposition over documents. Overall, LlamaIndex is designed to facilitate the development of LLM-enabled experiences, such as search engines and chatbots, by combining the capabilities of Weaviate and LlamaIndex.\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "response = await query_engine.aquery('How does LlamaIndex help data indexing in Weaviate?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_WThvqxJ0Go",
        "outputId": "281fe032-1f37-4d21-bc4d-b1d9d01b4a2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaIndex helps with data indexing in Weaviate by providing tools and capabilities for data ingestion, management, and querying. It offers connectors to various data sources, allowing users to easily integrate data from existing files and applications into Weaviate. LlamaIndex supports indexing unstructured, semi-structured, and structured data, enabling users to split source documents into text chunks and store them in Weaviate's vector database. This facilitates efficient and effective indexing of data in Weaviate, making it easier to retrieve specific facts, figures, or contextually relevant information when building applications such as search engines and chatbots.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
